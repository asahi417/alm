# BERT is to NLP what AlexNet is to CV
This is the official implementation of [***BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?***]()
which has been accepted by **[ACL 2021 main conference]**(https://2021.aclweb.org/). We evaluate pretrained language models on analogy test with new scoring function called,
*analogical proportion (AP)* scores and show 

<p align="center">
  <img src="asset/overview.png" width="500">
</p>   

- [experimental results](https://github.com/asahi417/alm/releases/download/0.0.0/experiments_results.tar.gz)


## Citation
Please cite our paper if you use our code or if you re-implement our method:
```
@inproceedings{ushio-etal-2021-bert-is,
    title = "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?",
    author = "Ushio, Asahi and
            Espinosa-Anke, Luis and 
            Schockaert, Steven and
            Camacho-Collados, Jose",
    booktitle = "Proceedings of the {ACL}-{IJCNLP} 2021 Conference",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics"
}
```
