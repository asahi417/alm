{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "otherwise-going",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "czech-sacramento",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-94f1391d36aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTransformersLM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\" transformers language model based sentence-mining \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-94f1391d36aa>\u001b[0m in \u001b[0;36mTransformersLM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running on {} GPU'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0minput_ids_to_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \"\"\" Labels generation for loss computation\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TransformersLM:\n",
    "    \"\"\" transformers language model based sentence-mining \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: str,\n",
    "                 max_length: int = None,\n",
    "                 cache_dir: str = './cache',\n",
    "                 num_worker: int = 1):\n",
    "        \"\"\" transformers language model based sentence-mining\n",
    "\n",
    "        :param model: a model name corresponding to a model card in `transformers`\n",
    "        :param max_length: a model max length if specified, else use model_max_length\n",
    "        \"\"\"\n",
    "        logging.debug('*** setting up a language model ***')\n",
    "        self.num_worker = num_worker\n",
    "        if self.num_worker == 1:\n",
    "            os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # to turn off warning message\n",
    "\n",
    "        self.model_type = None\n",
    "        self.model_name = model\n",
    "        self.cache_dir = cache_dir\n",
    "        self.device = 'cpu'\n",
    "        self.model = None\n",
    "        self.is_causal = 'gpt' in self.model_name  # TODO: fix to be more comprehensive method\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        if self.is_causal:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.config = transformers.AutoConfig.from_pretrained(model, cache_dir=cache_dir)\n",
    "        if max_length:\n",
    "            assert self.tokenizer.model_max_length >= max_length, '{} < {}'.format(self.tokenizer.model_max_length, max_length)\n",
    "            self.max_length = max_length\n",
    "        else:\n",
    "            self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "        # sentence prefix tokens\n",
    "        tokens = self.tokenizer.tokenize('get tokenizer specific prefix')\n",
    "        tokens_encode = self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode('get tokenizer specific prefix'))\n",
    "        self.sp_token_prefix = tokens_encode[:tokens_encode.index(tokens[0])]\n",
    "        self.sp_token_suffix = tokens_encode[tokens_encode.index(tokens[-1]) + 1:]\n",
    "\n",
    "    def load_model(self, lm_head: bool = True):\n",
    "        \"\"\" Model setup \"\"\"\n",
    "        logging.info('load language model')\n",
    "        params = dict(config=self.config, cache_dir=self.cache_dir)\n",
    "        if lm_head and self.is_causal:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_name, **params)\n",
    "            self.model_type = 'causal_lm'\n",
    "        elif lm_head:\n",
    "            self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name, **params)\n",
    "            self.model_type = 'masked_lm'\n",
    "        else:\n",
    "            self.model = transformers.AutoModel.from_pretrained(self.model_name, **params)\n",
    "            self.model_type = 'embedding'\n",
    "        self.model.eval()\n",
    "        # gpu\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        assert n_gpu <= 1\n",
    "        self.device = 'cuda' if n_gpu > 0 else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        logging.info('running on {} GPU'.format(n_gpu))\n",
    "\n",
    "    def input_ids_to_labels(self, input_ids, label_position: List = None, label_id: List = None):\n",
    "        \"\"\" Labels generation for loss computation\n",
    "\n",
    "        :param input_ids: input_ids given by tokenizer.encode\n",
    "        :param label_position: position to keep for label\n",
    "        :param label_id: indices to use in `label_position`\n",
    "        :return: labels, a list of indices for loss computation\n",
    "        \"\"\"\n",
    "        if label_position is None and label_id is None:\n",
    "            # ignore padding token\n",
    "            label = list(map(lambda x: PAD_TOKEN_LABEL_ID if x == self.tokenizer.pad_token_id else x, input_ids))\n",
    "        else:\n",
    "            assert len(label_position) == len(label_id)\n",
    "            label = [PAD_TOKEN_LABEL_ID] * len(input_ids)\n",
    "            for p, i in zip(label_position, label_id):\n",
    "                label[p] = i\n",
    "        if self.is_causal:  # shift the label sequence for causal inference\n",
    "            label = label[1:] + [PAD_TOKEN_LABEL_ID]\n",
    "        return label\n",
    "\n",
    "    def __get_nll(self, data_loader, reduce: bool = True):\n",
    "        \"\"\" Negative log likelihood (NLL)\n",
    "\n",
    "        :param data_loader: data loader\n",
    "        :param reduce: to reduce NLL over sequence or not\n",
    "        :return: a list of NLL\n",
    "        \"\"\"\n",
    "        assert self.model\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        nll = []\n",
    "        with torch.no_grad():\n",
    "            for encode in tqdm(data_loader):\n",
    "                encode = {k: v.to(self.device) for k, v in encode.items()}\n",
    "                labels = encode.pop('labels')\n",
    "                output = self.model(**encode, return_dict=True)\n",
    "                prediction_scores = output['logits']\n",
    "                loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "                loss = loss.view(len(prediction_scores), -1)\n",
    "\n",
    "                if reduce:\n",
    "                    loss = torch.sum(loss, -1)\n",
    "                    nll += list(map(\n",
    "                        lambda x: x[0] / sum(map(lambda y: y != PAD_TOKEN_LABEL_ID, x[1])),\n",
    "                        zip(loss.cpu().tolist(), labels.cpu().tolist())\n",
    "                    ))\n",
    "                else:\n",
    "                    nll += list(map(\n",
    "                        lambda x: list(map(\n",
    "                            lambda y: y[1],\n",
    "                            filter(lambda z: z[0] != PAD_TOKEN_LABEL_ID, zip(x[0], x[1]))\n",
    "                        )),\n",
    "                        zip(labels.cpu().tolist(), loss.cpu().tolist())))\n",
    "\n",
    "        return nll\n",
    "\n",
    "    ########################################\n",
    "    # Modules for negative PMI computation #\n",
    "    ########################################\n",
    "    def encode_plus_mask(self,\n",
    "                         word: List,\n",
    "                         template_type: str,\n",
    "                         mask_index: int,\n",
    "                         mask_index_no_label: int = None):\n",
    "        \"\"\" An output from `encode_plus` with a masked token specified by a string with a `labels` indicating\n",
    "        the masking position as the masked token id otherwise `PAD_TOKEN_LABEL_ID`\n",
    "        * Token with multiple sub-words includes all the possible decoding paths\n",
    "        \"\"\"\n",
    "        assert not self.is_causal\n",
    "        text, position = prompting_relation(word, template_type=template_type)\n",
    "        token_list = self.tokenizer.tokenize(text)\n",
    "        token_list_tmp = token_list.copy()\n",
    "        if mask_index_no_label is not None:\n",
    "            s, e = find_position(self.tokenizer, position[mask_index_no_label], text, token_list)\n",
    "            token_list_tmp[s:e] = [self.tokenizer.mask_token] * (e - s)\n",
    "\n",
    "        # print(position, mask_index)\n",
    "        s, e = find_position(self.tokenizer, position[mask_index], text, token_list)\n",
    "        all_encode = self.encode_combinations(token_list_tmp, list(range(s, e)))\n",
    "        return DictKeeper(all_encode, target_key='encode')\n",
    "\n",
    "    def encode_combinations(self, token_list, positions: List):\n",
    "        \"\"\" Encode all the combination of positions\n",
    "\n",
    "        :param token_list: a list of token\n",
    "        :param positions: a list of position (index)\n",
    "        :return: a nested dictionary consisting of masked encoding with all position patterns\n",
    "        \"\"\"\n",
    "\n",
    "        def pop(__list, value):\n",
    "            __list = __list.copy()\n",
    "            __list.pop(__list.index(value))\n",
    "            _out = {\"index\": __list, \"encode\": self.encode_position(token_list, __list)}\n",
    "            if len(__list) > 1:\n",
    "                _out['child'] = {_i: pop(__list, _i) for _i in __list}\n",
    "            return _out\n",
    "\n",
    "        assert len(positions) != 0\n",
    "        out = {\"index\": positions, \"encode\": self.encode_position(token_list, positions)}\n",
    "        if len(positions) != 1:\n",
    "            out['child'] = {i: pop(positions, i) for i in positions}\n",
    "        return out\n",
    "\n",
    "    def encode_position(self, token_list, position: List):\n",
    "        \"\"\" Encode tokens with masks at a position\n",
    "\n",
    "        :param token_list: a list of token\n",
    "        :param position: a position eg) (1, 2)\n",
    "        :return: encode with labels by `token_list` with masks at `_positions`\n",
    "        \"\"\"\n",
    "        param = {'max_length': self.max_length, 'truncation': True, 'padding': 'max_length'}\n",
    "        tmp_token_list = token_list.copy()\n",
    "        label_id = []\n",
    "        for _p in position:\n",
    "            label_id.append(self.tokenizer.convert_tokens_to_ids(tmp_token_list[_p]))\n",
    "            tmp_token_list[_p] = self.tokenizer.mask_token\n",
    "        tmp_string = self.tokenizer.convert_tokens_to_string(tmp_token_list)\n",
    "        _encode = self.tokenizer.encode_plus(tmp_string, **param)\n",
    "        _encode['labels'] = self.input_ids_to_labels(\n",
    "            _encode['input_ids'], label_position=position, label_id=label_id)\n",
    "        return _encode\n",
    "\n",
    "    def batch_encode_plus_mask(self,\n",
    "                               template_type: str,\n",
    "                               batch_word: List,\n",
    "                               batch_mask_index: List,\n",
    "                               batch_mask_index_no_label: List = None,\n",
    "                               batch_size: int = None):\n",
    "        \"\"\" Batch version of `self.encode_plus_mask`\n",
    "\n",
    "        :param batch_size: batch size\n",
    "        :return: (`torch.utils.data.DataLoader` class, partition)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = len(batch_word) if batch_size is None else batch_size\n",
    "        if batch_mask_index_no_label is None:\n",
    "            batch_mask_index_no_label = [None] * len(batch_word)\n",
    "\n",
    "        assert len(batch_word) == len(batch_mask_index) == len(batch_mask_index_no_label)\n",
    "\n",
    "        logging.info('creating data loader')\n",
    "        data_dk = []\n",
    "        data_flat = []\n",
    "        # this can be parallelized, but due to deepcopy at DictKeeper, it may cause memory error in some machine\n",
    "        for x in tqdm(list(zip(batch_word, batch_mask_index, batch_mask_index_no_label))):\n",
    "            tmp = self.encode_plus_mask(\n",
    "                word=x[0], mask_index=x[1], mask_index_no_label=x[2], template_type=template_type)\n",
    "            data_dk.append(tmp)\n",
    "            data_flat.append(tmp.flat_values)\n",
    "        partition = get_partition(data_flat)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            Dataset(list(chain(*data_flat))),\n",
    "            num_workers=self.num_worker, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "        )\n",
    "        return data_loader, partition, data_dk\n",
    "\n",
    "    def get_negative_pmi(self,\n",
    "                         template_type: str,\n",
    "                         word: List,\n",
    "                         mask_index: List,\n",
    "                         mask_index_condition: List = None,\n",
    "                         batch_size: int = None,\n",
    "                         weight: float = None):\n",
    "        \"\"\" Negative Point-wise Mutual Information (PMI)\n",
    "        negative PMI(t|c) = - PMI(t|c) = - (w * sum[NLL(t)] - sum[NLL(t|c=mask])])\n",
    "        negative PMI(t|c) = w * sum[NLL(t)] - sum[NLL(t|c=mask])]\n",
    "        - NLL(t): negative log likelihood of t\n",
    "        - t: objective (sub) tokens\n",
    "        - c: conditioning (sub) tokens\n",
    "        - w: conditional NLL weight\n",
    "        Sum over (sub) tokens are based on lowest NLL search\n",
    "\n",
    "        :param batch_size: batch size\n",
    "        :param weight: conditional NLL weight\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert type(word) is list and type(mask_index) is list, 'type error'\n",
    "        if not self.model:\n",
    "            self.load_model()\n",
    "        assert self.model_type != 'embedding'\n",
    "        weight = 1 if weight is None else weight\n",
    "\n",
    "        def decode_score(_nested_score, total_score: float = 0.0):\n",
    "            \"\"\" Lowest nll based subword decoding \"\"\"\n",
    "            scores = _nested_score['score']\n",
    "            if len(scores) == 1:\n",
    "                assert 'child' not in _nested_score.keys()\n",
    "                return total_score + scores[0]\n",
    "            else:\n",
    "                assert 'child' in _nested_score.keys()\n",
    "                assert len(_nested_score['score']) == len(_nested_score['child']) == len(_nested_score['index'])\n",
    "                best_score = min(_nested_score['score'])\n",
    "                best_i = _nested_score['index'][_nested_score['score'].index(best_score)]\n",
    "                return decode_score(_nested_score['child'][best_i], total_score + best_score)\n",
    "\n",
    "        data_loader, partition, data_dk = self.batch_encode_plus_mask(\n",
    "            template_type=template_type,\n",
    "            batch_word=word,\n",
    "            batch_mask_index=mask_index,\n",
    "            batch_size=batch_size)\n",
    "        logging.info('inference')\n",
    "        score = self.__get_nll(data_loader, reduce=False)\n",
    "        conditional_nll = list(map(\n",
    "            lambda x: decode_score(x[0].restore_structure(score[x[1][0]:x[1][1]], insert_key='score')),\n",
    "            zip(data_dk, partition)\n",
    "        ))\n",
    "        if mask_index_condition:\n",
    "            data_loader, partition, data_dk = self.batch_encode_plus_mask(\n",
    "                template_type=template_type,\n",
    "                batch_word=word,\n",
    "                batch_mask_index=mask_index,\n",
    "                batch_mask_index_no_label=mask_index_condition,\n",
    "                batch_size=batch_size)\n",
    "            score = self.__get_nll(data_loader, reduce=False)\n",
    "            marginal_nll = list(map(\n",
    "                lambda x: decode_score(x[0].restore_structure(score[x[1][0]:x[1][1]], insert_key='score')),\n",
    "                zip(data_dk, partition)\n",
    "            ))\n",
    "            assert len(conditional_nll) == len(marginal_nll)\n",
    "            negative_pmi = list(map(lambda x: x[0] * weight - x[1], zip(conditional_nll, marginal_nll)))\n",
    "            return negative_pmi\n",
    "        else:\n",
    "            return conditional_nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"stem\": [\"beauty\", \"aesthete\"], \"answer\": 0, \"choice\": [[\"pleasure\", \"hedonist\"], [\"emotion\", \"demagogue\"], [\"opinion\", \"sympathizer\"], [\"seance\", \"medium\"], [\"luxury\", \"ascetic\"]], \"prefix\": \"190 FROM REAL SATs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "downtown-opera",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauty', 'aesthete']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['stem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broad-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pleasure', 'hedonist'],\n",
       " ['emotion', 'demagogue'],\n",
       " ['opinion', 'sympathizer'],\n",
       " ['seance', 'medium'],\n",
       " ['luxury', 'ascetic']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['choice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-breeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
