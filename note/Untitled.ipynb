{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-canberra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "another-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "from itertools import chain\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # to turn off warning message\n",
    "PAD_TOKEN_LABEL_ID = nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "\n",
    "def get_partition(_list):\n",
    "    length = list(map(lambda x: len(x), _list))\n",
    "    return list(map(lambda x: [sum(length[:x]), sum(length[:x + 1])], range(len(length))))\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" `torch.utils.data.Dataset` \"\"\"\n",
    "    float_tensors = ['attention_mask']\n",
    "\n",
    "    def __init__(self, data: List):\n",
    "        self.data = data  # a list of dictionaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def to_tensor(self, name, data):\n",
    "        if name in self.float_tensors:\n",
    "            return torch.tensor(data, dtype=torch.float32)\n",
    "        return torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: self.to_tensor(k, v) for k, v in self.data[idx].items()}\n",
    "\n",
    "\n",
    "class TransformersLM:\n",
    "    \"\"\" transformers language model based sentence-mining \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: str,\n",
    "                 max_length: int = None,\n",
    "                 cache_dir: str = './cache',\n",
    "                 num_worker: int = 0):\n",
    "        \"\"\" transformers language model based sentence-mining\n",
    "\n",
    "        :param model: a model name corresponding to a model card in `transformers`\n",
    "        :param max_length: a model max length if specified, else use model_max_length\n",
    "        \"\"\"\n",
    "        logging.debug('*** setting up a language model ***')\n",
    "        self.num_worker = num_worker\n",
    "        if self.num_worker == 1:\n",
    "            os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # to turn off warning message\n",
    "\n",
    "        self.model_type = None\n",
    "        self.model_name = model\n",
    "        self.cache_dir = cache_dir\n",
    "        self.device = 'cpu'\n",
    "        self.model = None\n",
    "        self.is_causal = 'gpt' in self.model_name  # TODO: fix to be more comprehensive method\n",
    "        assert not self.is_causal\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        if self.is_causal:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.config = transformers.AutoConfig.from_pretrained(model, cache_dir=cache_dir)\n",
    "        if max_length:\n",
    "            assert self.tokenizer.model_max_length >= max_length, '{} < {}'.format(self.tokenizer.model_max_length,\n",
    "                                                                                   max_length)\n",
    "            self.max_length = max_length\n",
    "        else:\n",
    "            self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "        # sentence prefix tokens\n",
    "        tokens = self.tokenizer.tokenize('get tokenizer specific prefix')\n",
    "        tokens_encode = self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode('get tokenizer specific prefix'))\n",
    "        self.sp_token_prefix = tokens_encode[:tokens_encode.index(tokens[0])]\n",
    "        self.sp_token_suffix = tokens_encode[tokens_encode.index(tokens[-1]) + 1:]\n",
    "\n",
    "    def input_ids_to_labels(self, input_ids, label_position: List = None, label_id: List = None):\n",
    "        \"\"\" Generate label for likelihood computation\n",
    "\n",
    "        :param input_ids: input_ids given by tokenizer.encode\n",
    "        :param label_position: position to keep for label\n",
    "        :param label_id: indices to use in `label_position`\n",
    "        :return: labels, a list of indices for loss computation\n",
    "        \"\"\"\n",
    "        if label_position is None and label_id is None:\n",
    "            # ignore padding token\n",
    "            label = list(map(lambda x: PAD_TOKEN_LABEL_ID if x == self.tokenizer.pad_token_id else x, input_ids))\n",
    "        else:\n",
    "            assert len(label_position) == len(label_id)\n",
    "            label = [PAD_TOKEN_LABEL_ID] * len(input_ids)\n",
    "            for p, i in zip(label_position, label_id):\n",
    "                label[p] = i\n",
    "        if self.is_causal:  # shift the label sequence for causal inference\n",
    "            label = label[1:] + [PAD_TOKEN_LABEL_ID]\n",
    "        return label\n",
    "\n",
    "    def cleanup_decode(self, sentence):\n",
    "        to_remove = list(filter(lambda x: x != self.tokenizer.mask_token, self.tokenizer.all_special_tokens))\n",
    "        to_remove = '|'.join(to_remove).replace('[', '\\[').replace(']', '\\]')\n",
    "        sentence = re.sub(r'{}'.format(to_remove), '', sentence)\n",
    "        sentence = re.sub(r'\\A\\s*', '', sentence)\n",
    "        return sentence\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\" Model setup \"\"\"\n",
    "        logging.info('load language model')\n",
    "        params = dict(config=self.config, cache_dir=self.cache_dir)\n",
    "        if self.is_causal:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_name, **params)\n",
    "            self.model_type = 'causal_lm'\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name, **params)\n",
    "            self.model_type = 'masked_lm'\n",
    "        self.model.eval()\n",
    "        # gpu\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        assert n_gpu <= 1\n",
    "        self.device = 'cuda' if n_gpu > 0 else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        logging.info('running on {} GPU'.format(n_gpu))\n",
    "\n",
    "    def pair_to_seed(self, word_pair: List, n_blank: int = 3):\n",
    "        assert len(word_pair) == 2, '{}'.format(len(word_pair))\n",
    "        h, t = word_pair\n",
    "        return ' '.join([h] + [self.tokenizer.mask_token] * n_blank + [t])\n",
    "\n",
    "    def encode_plus(self, sentence):\n",
    "        param = {'max_length': self.max_length, 'truncation': True, 'padding': 'max_length'}\n",
    "        encode = self.tokenizer.encode_plus(sentence, **param)\n",
    "        assert self.tokenizer.mask_token_id in encode['input_ids']\n",
    "        encode['mask_flag'] = list(map(lambda x: int(x == self.tokenizer.mask_token_id), encode['input_ids']))\n",
    "        return encode\n",
    "\n",
    "    def replace_mask(self,\n",
    "                     word_pairs: List,\n",
    "                     n_blank: int = 3,\n",
    "                     topk: int = 5,\n",
    "                     batch_size: int = 4,\n",
    "                     perplexity_filter: bool = True,\n",
    "                     debug: bool = False):\n",
    "        if type(word_pairs[0]) is not list:\n",
    "            word_pairs = [word_pairs]\n",
    "        seed_sentences = list(map(lambda x: self.pair_to_seed(x, n_blank), word_pairs))\n",
    "        shared = dict(topk=topk, debug=debug, batch_size=batch_size, perplexity_filter=perplexity_filter)\n",
    "        for i in range(n_blank):\n",
    "            seed_sentences = self.replace_single_mask(seed_sentences, **shared)\n",
    "        return seed_sentences\n",
    "\n",
    "    def replace_single_mask(self,\n",
    "                            seed_sentences,\n",
    "                            batch_size: int = 4,\n",
    "                            topk: int = 5,\n",
    "                            perplexity_filter: bool = True,\n",
    "                            debug: bool = False):\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        if type(seed_sentences) is str:\n",
    "            seed_sentences = [seed_sentences]\n",
    "\n",
    "        data = list(map(self.encode_plus, seed_sentences))\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            Dataset(data),\n",
    "            num_workers=self.num_worker,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False)\n",
    "\n",
    "        logging.info('Inference on masked token')\n",
    "        total_input = []\n",
    "        total_mask = []\n",
    "        total_val = []  # batch, mask_size, topk\n",
    "        total_ind = []\n",
    "        with torch.no_grad():\n",
    "            for encode in tqdm(data_loader):\n",
    "                encode = {k: v.to(self.device) for k, v in encode.items()}\n",
    "                mask_flag = encode.pop('mask_flag')\n",
    "                output = self.model(**encode, return_dict=True)\n",
    "                prediction_scores = output['logits']\n",
    "                values, indices = prediction_scores.topk(topk, dim=-1)\n",
    "                total_input += encode.pop('input_ids').tolist()\n",
    "                total_mask += mask_flag.tolist()\n",
    "                total_val += values.tolist()\n",
    "                total_ind += indices.tolist()\n",
    "\n",
    "        def edit_input(batch_i):\n",
    "            inp, mas, val, ind = total_input[batch_i], total_mask[batch_i], total_val[batch_i], total_ind[batch_i]\n",
    "            filtered = list(filter(lambda x: mas[x[0]] == 1, enumerate(zip(val, ind))))\n",
    "            # to replace the position with the highest likelihood among possible masked positions\n",
    "            replace_pos, (_, ind) = sorted(filtered, key=lambda x: x[1][0][0], reverse=True)[0]\n",
    "\n",
    "            def decode_topk(k):\n",
    "                inp_ = deepcopy(inp)\n",
    "                inp_[replace_pos] = ind[k]\n",
    "                decoded = self.tokenizer.decode(inp_, skip_special_tokens=False)\n",
    "                return self.cleanup_decode(decoded)\n",
    "\n",
    "            topk_decoded = list(map(decode_topk, range(topk)))\n",
    "            return topk_decoded\n",
    "\n",
    "        greedy_filling = list(map(edit_input, range(len(total_input))))\n",
    "        if perplexity_filter:\n",
    "            logging.info('ppl filtering')\n",
    "            best_edit = []\n",
    "            for s in greedy_filling:\n",
    "                ppl = self.get_perplexity(s)\n",
    "                best_edit.append(s[ppl.index(max(ppl))])\n",
    "        else:\n",
    "            best_edit = list(map(lambda x: x[0], greedy_filling))\n",
    "\n",
    "        if debug:\n",
    "            for o, e in zip(seed_sentences, best_edit):\n",
    "                logging.info('\\n- original: {}\\n- edit : {}\\n'.format(o, e))\n",
    "        return best_edit\n",
    "\n",
    "    def encode_plus_perplexity(self, sentence):\n",
    "        \"\"\" An output from `encode_plus` for perplexity computation\n",
    "        * for pseudo perplexity, encode all text with mask on every token one by one\n",
    "        \"\"\"\n",
    "        param = {'max_length': self.max_length, 'truncation': True, 'padding': 'max_length'}\n",
    "        if self.is_causal:\n",
    "            encode = self.tokenizer.encode_plus(sentence, **param)\n",
    "            encode['labels'] = self.input_ids_to_labels(encode['input_ods'])\n",
    "            return [encode]\n",
    "        else:\n",
    "            token_list = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "            def encode_with_single_mask_id(mask_position: int):\n",
    "                _token_list = token_list.copy()  # can not be encode outputs because of prefix\n",
    "                masked_token_id = self.tokenizer.convert_tokens_to_ids(_token_list[mask_position])\n",
    "                _token_list[mask_position] = self.tokenizer.mask_token\n",
    "                tmp_string = self.tokenizer.convert_tokens_to_string(_token_list)\n",
    "                _encode = self.tokenizer.encode_plus(tmp_string, **param)\n",
    "                _encode['labels'] = self.input_ids_to_labels(\n",
    "                    _encode['input_ids'],\n",
    "                    label_position=[mask_position + len(self.sp_token_prefix)],\n",
    "                    label_id=[masked_token_id])\n",
    "                return _encode\n",
    "\n",
    "            length = min(self.max_length - len(self.sp_token_prefix), len(token_list))\n",
    "            return list(map(encode_with_single_mask_id, range(length)))\n",
    "\n",
    "    def get_perplexity(self, sentences, batch_size: int = 64):\n",
    "        \"\"\" compute perplexity on each sentence\n",
    "\n",
    "        :param batch_size:\n",
    "        :param sentences:\n",
    "        :return: a list of perplexity\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        if type(sentences) is str:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        data = list(map(self.encode_plus_perplexity, sentences))\n",
    "        partition = get_partition(data)\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            Dataset(list(chain(*data))),\n",
    "            num_workers=self.num_worker, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "        )\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        nll = []\n",
    "        with torch.no_grad():\n",
    "            for encode in tqdm(data_loader):\n",
    "                encode = {k: v.to(self.device) for k, v in encode.items()}\n",
    "                labels = encode.pop('labels')\n",
    "                output = self.model(**encode, return_dict=True)\n",
    "                prediction_scores = output['logits']\n",
    "                loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "                loss = loss.view(len(prediction_scores), -1)\n",
    "                loss = torch.sum(loss, -1)\n",
    "                nll += list(map(\n",
    "                    lambda x: x[0] / sum(map(lambda y: y != PAD_TOKEN_LABEL_ID, x[1])),\n",
    "                    zip(loss.cpu().tolist(), labels.cpu().tolist())\n",
    "                ))\n",
    "        # return nll, partition\n",
    "        perplexity = list(map(lambda x: math.exp(sum(nll[x[0]:x[1]]) / (x[1] - x[0])), partition))\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "premier-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = TransformersLM('albert-base-v1', max_length=32, num_worker=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "superb-prime",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-06 22:13:24 INFO     load language model\n",
      "2021-02-06 22:13:25 INFO     running on 0 GPU\n",
      "2021-02-06 22:13:25 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.67it/s]\n",
      "2021-02-06 22:13:25 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "2021-02-06 22:13:28 INFO     \n",
      "- original: pleasure [MASK] [MASK] [MASK] hedonist\n",
      "- editted : pleasure and[MASK][MASK] hedonist\n",
      "\n",
      "2021-02-06 22:13:28 INFO     \n",
      "- original: emotion [MASK] [MASK] [MASK] demagogue\n",
      "- editted : emotion of[MASK][MASK] demagogue\n",
      "\n",
      "2021-02-06 22:13:28 INFO     \n",
      "- original: opinion [MASK] [MASK] [MASK] sympathizer\n",
      "- editted : opinion:[MASK][MASK] sympathizer\n",
      "\n",
      "2021-02-06 22:13:28 INFO     \n",
      "- original: seance [MASK] [MASK] [MASK] medium\n",
      "- editted : seance[MASK][MASK] light medium\n",
      "\n",
      "2021-02-06 22:13:28 INFO     \n",
      "- original: luxury [MASK] [MASK] [MASK] ascetic\n",
      "- editted : luxury[MASK] the[MASK] ascetic\n",
      "\n",
      "2021-02-06 22:13:28 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.40it/s]\n",
      "2021-02-06 22:13:28 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "2021-02-06 22:13:33 INFO     \n",
      "- original: pleasure and[MASK][MASK] hedonist\n",
      "- editted : pleasure and love[MASK] hedonist\n",
      "\n",
      "2021-02-06 22:13:33 INFO     \n",
      "- original: emotion of[MASK][MASK] demagogue\n",
      "- editted : emotion of the[MASK] demagogue\n",
      "\n",
      "2021-02-06 22:13:33 INFO     \n",
      "- original: opinion:[MASK][MASK] sympathizer\n",
      "- editted : opinion:[MASK] american sympathizer\n",
      "\n",
      "2021-02-06 22:13:33 INFO     \n",
      "- original: seance[MASK][MASK] light medium\n",
      "- editted : seance [MASK] light medium\n",
      "\n",
      "2021-02-06 22:13:33 INFO     \n",
      "- original: luxury[MASK] the[MASK] ascetic\n",
      "- editted : luxury and the[MASK] ascetic\n",
      "\n",
      "2021-02-06 22:13:33 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.66it/s]\n",
      "2021-02-06 22:13:33 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "2021-02-06 22:13:37 INFO     \n",
      "- original: pleasure and love[MASK] hedonist\n",
      "- editted : pleasure and love the hedonist\n",
      "\n",
      "2021-02-06 22:13:37 INFO     \n",
      "- original: emotion of the[MASK] demagogue\n",
      "- editted : emotion of the old demagogue\n",
      "\n",
      "2021-02-06 22:13:37 INFO     \n",
      "- original: opinion:[MASK] american sympathizer\n",
      "- editted : opinion: the american sympathizer\n",
      "\n",
      "2021-02-06 22:13:37 INFO     \n",
      "- original: seance [MASK] light medium\n",
      "- editted : seance & light medium\n",
      "\n",
      "2021-02-06 22:13:37 INFO     \n",
      "- original: luxury and the[MASK] ascetic\n",
      "- editted : luxury and the spirit ascetic\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasure and love the hedonist', 'emotion of the old demagogue', 'opinion: the american sympathizer', 'seance & light medium', 'luxury and the spirit ascetic']\n"
     ]
    }
   ],
   "source": [
    "sample = {\"stem\": [\"beauty\", \"aesthete\"], \"answer\": 0, \"choice\": [[\"pleasure\", \"hedonist\"], [\"emotion\", \"demagogue\"], [\"opinion\", \"sympathizer\"], [\"seance\", \"medium\"], [\"luxury\", \"ascetic\"]], \"prefix\": \"190 FROM REAL SATs\"}\n",
    "o = lm.replace_mask(sample['choice'], debug=True, perplexity_filter=True, topk=5)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "domestic-relation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-06 22:13:37 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.19it/s]\n",
      "2021-02-06 22:13:37 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "2021-02-06 22:13:43 INFO     \n",
      "- original: pleasure [MASK] [MASK] [MASK] [MASK] [MASK] hedonist\n",
      "- editted : pleasure +[MASK][MASK][MASK][MASK] hedonist\n",
      "\n",
      "2021-02-06 22:13:43 INFO     \n",
      "- original: emotion [MASK] [MASK] [MASK] [MASK] [MASK] demagogue\n",
      "- editted : emotion de[MASK][MASK][MASK][MASK] demagogue\n",
      "\n",
      "2021-02-06 22:13:43 INFO     \n",
      "- original: opinion [MASK] [MASK] [MASK] [MASK] [MASK] sympathizer\n",
      "- editted : opinion red[MASK][MASK][MASK][MASK] sympathizer\n",
      "\n",
      "2021-02-06 22:13:43 INFO     \n",
      "- original: seance [MASK] [MASK] [MASK] [MASK] [MASK] medium\n",
      "- editted : seance +[MASK][MASK][MASK][MASK] medium\n",
      "\n",
      "2021-02-06 22:13:43 INFO     \n",
      "- original: luxury [MASK] [MASK] [MASK] [MASK] [MASK] ascetic\n",
      "- editted : luxury +[MASK][MASK][MASK][MASK] ascetic\n",
      "\n",
      "2021-02-06 22:13:43 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.08it/s]\n",
      "2021-02-06 22:13:43 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "2021-02-06 22:13:49 INFO     \n",
      "- original: pleasure +[MASK][MASK][MASK][MASK] hedonist\n",
      "- editted : pleasure +[MASK][MASK][MASK] the hedonist\n",
      "\n",
      "2021-02-06 22:13:49 INFO     \n",
      "- original: emotion de[MASK][MASK][MASK][MASK] demagogue\n",
      "- editted : emotion de[MASK][MASK][MASK] del demagogue\n",
      "\n",
      "2021-02-06 22:13:49 INFO     \n",
      "- original: opinion red[MASK][MASK][MASK][MASK] sympathizer\n",
      "- editted : opinion red[MASK][MASK][MASK] red sympathizer\n",
      "\n",
      "2021-02-06 22:13:49 INFO     \n",
      "- original: seance +[MASK][MASK][MASK][MASK] medium\n",
      "- editted : seance +[MASK][MASK][MASK] 2 medium\n",
      "\n",
      "2021-02-06 22:13:49 INFO     \n",
      "- original: luxury +[MASK][MASK][MASK][MASK] ascetic\n",
      "- editted : luxury +[MASK][MASK][MASK] and ascetic\n",
      "\n",
      "2021-02-06 22:13:49 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.08it/s]\n",
      "2021-02-06 22:13:49 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "2021-02-06 22:13:55 INFO     \n",
      "- original: pleasure +[MASK][MASK][MASK] the hedonist\n",
      "- editted : pleasure +[MASK][MASK]: the hedonist\n",
      "\n",
      "2021-02-06 22:13:55 INFO     \n",
      "- original: emotion de[MASK][MASK][MASK] del demagogue\n",
      "- editted : emotion de[MASK][MASK]ciones del demagogue\n",
      "\n",
      "2021-02-06 22:13:55 INFO     \n",
      "- original: opinion red[MASK][MASK][MASK] red sympathizer\n",
      "- editted : opinion red press[MASK][MASK] red sympathizer\n",
      "\n",
      "2021-02-06 22:13:55 INFO     \n",
      "- original: seance +[MASK][MASK][MASK] 2 medium\n",
      "- editted : seance +[MASK][MASK] & 2 medium\n",
      "\n",
      "2021-02-06 22:13:55 INFO     \n",
      "- original: luxury +[MASK][MASK][MASK] and ascetic\n",
      "- editted : luxury +[MASK]:[MASK] and ascetic\n",
      "\n",
      "2021-02-06 22:13:55 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.05it/s]\n",
      "2021-02-06 22:13:55 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "2021-02-06 22:14:01 INFO     \n",
      "- original: pleasure +[MASK][MASK]: the hedonist\n",
      "- editted : pleasure + the[MASK] : the hedonist\n",
      "\n",
      "2021-02-06 22:14:01 INFO     \n",
      "- original: emotion de[MASK][MASK]ciones del demagogue\n",
      "- editted : emotion de[MASK]: ciones del demagogue\n",
      "\n",
      "2021-02-06 22:14:01 INFO     \n",
      "- original: opinion red press[MASK][MASK] red sympathizer\n",
      "- editted : opinion red press[MASK] red red sympathizer\n",
      "\n",
      "2021-02-06 22:14:01 INFO     \n",
      "- original: seance +[MASK][MASK] & 2 medium\n",
      "- editted : seance + 1[MASK] & 2 medium\n",
      "\n",
      "2021-02-06 22:14:01 INFO     \n",
      "- original: luxury +[MASK]:[MASK] and ascetic\n",
      "- editted : luxury +[MASK] : honest and ascetic\n",
      "\n",
      "2021-02-06 22:14:01 INFO     Inference on masked token\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.36it/s]\n",
      "2021-02-06 22:14:01 INFO     ppl filtering\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "2021-02-06 22:14:08 INFO     \n",
      "- original: pleasure + the[MASK] : the hedonist\n",
      "- editted : pleasure + the spirit : the hedonist\n",
      "\n",
      "2021-02-06 22:14:08 INFO     \n",
      "- original: emotion de[MASK]: ciones del demagogue\n",
      "- editted : emotion de futbol : ciones del demagogue\n",
      "\n",
      "2021-02-06 22:14:08 INFO     \n",
      "- original: opinion red press[MASK] red red sympathizer\n",
      "- editted : opinion red press bulletin red red sympathizer\n",
      "\n",
      "2021-02-06 22:14:08 INFO     \n",
      "- original: seance + 1[MASK] & 2 medium\n",
      "- editted : seance + 1 sample & 2 medium\n",
      "\n",
      "2021-02-06 22:14:08 INFO     \n",
      "- original: luxury +[MASK] : honest and ascetic\n",
      "- editted : luxury + entertainment : honest and ascetic\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasure + the spirit : the hedonist', 'emotion de futbol : ciones del demagogue', 'opinion red press bulletin red red sympathizer', 'seance + 1 sample & 2 medium', 'luxury + entertainment : honest and ascetic']\n"
     ]
    }
   ],
   "source": [
    "sample = {\"stem\": [\"beauty\", \"aesthete\"], \"answer\": 0, \"choice\": [[\"pleasure\", \"hedonist\"], [\"emotion\", \"demagogue\"], [\"opinion\", \"sympathizer\"], [\"seance\", \"medium\"], [\"luxury\", \"ascetic\"]], \"prefix\": \"190 FROM REAL SATs\"}\n",
    "o = lm.replace_mask(sample['choice'], debug=True, perplexity_filter=True, topk=5, n_blank=5)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "thorough-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasure the they hedonist',\n",
       " 'emotion the of the demagogue',\n",
       " 'opinion the red cross sympathizer',\n",
       " 'seance & & & medium',\n",
       " 'luxury luxury luxury luxury ascetic']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-tower",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
